
To install a package that does not come with the provided environment, please use the following steps:

Install your desired package using the following code:
`install.packages("PackageName", lib="../work/")`
Attach the library you just installed using the following code:
`library(PackageName, lib.loc = "../work/")`
**NOTE:** Make sure the file location is the same as the above code snippets `("../work/")`
Instructions for using the safe_read_csv() function
Purpose: The safe_read_csv() function is designed to verify the existence of datasets for specific questions.

Execution:

**Run the function to attempt loading the dataset into your Vocareum notebook.This function only needs to be run once each time you open the notebook** :
If the dataset loads successfully, you will see a "...successfully read" message. Please proceed with the rest of the questions.
If the dataset is missing, an error message stating "…not found…" will appear.Please report this issue by posting on Piazza so we can resolve it.


```{r}
# The safe_read_csv function is used for checking datasets.
safe_read_csv <- function(file_path, stringsAsFactors = TRUE) {
    result <- tryCatch({
        data <- read.csv(file_path, stringsAsFactors = stringsAsFactors)
        mes <- paste(file_path, " successfully read.")
        print(mes)
        return(data)
      }, warning = function(w) {
    # Warning Message
        message("Warning: ", conditionMessage(w))
        message("Using alternative file path.")
        # Use regular expression to get file name after last slash
        file_name <- sub("^.*/", "", file_path) 
        # See if file in first layer of working directory.
        # If not, student can easily upload it to Vocareum directory.
        result <- read.csv(file_name, stringsAsFactors = stringsAsFactors)
        print("Alternate path successful")
        return(result)
    }, error = function(e) {
    # If neither method was successful
        message("Error: ", conditionMessage(e))
        print("Please upload data to Vocareum")
      }, finally = {
        ## This message appears regardless of whether an error appears
        final_mes <- paste("If ", file_path, " not found, please notify us in Piazza. Thank you.")
        print(final_mes)
    }) 
    return(result)
}
```

## Question 1 (20 Points)
ABC Lab, a manufacturer of cement, is currently developing a new production process. As part of this initiative, the lab routinely creates multiple cement samples daily to evaluate their strength. The provided dataset comprises the strength measurements (in MPa) of eight randomly selected samples from the production line, recorded daily over a period of 59 days. ABC Lab intends to utilize statistical control charts to determine whether the process remains within controlled parameters.

Please use the 'concrete.csv' dataset for this analysis. The dataset includes these columns:

Date: The sequential date identifier for each observation.
Obs1 - Obs8: Measurement of strengths of the 8 randomly selected samples.

```{r}
## read dataset
#data_q1 = read.csv("../resource/asnlib/publicdata/concrete.csv", stringsAsFactors = FALSE) 
data_q1 = safe_read_csv("../resource/asnlib/publicdata/concrete.csv", stringsAsFactors = FALSE) 
str(data_q1)
head(data_q1)
```

### A. Calculate the following control limits: UCLx, LCLx, UCLr, LCLr (8 points)
Note: Use the following values: A_2 = 0.373, D_3 = 0.136, D_4 = 1.864.

```{r}

data_q1$X_bar = rowMeans(data_q1[,3:10])
data_q1$R = apply(data_q1[,3:10], 1, max)-apply(data_q1[,3:10], 1, min)

# Calculate X-double-bar and R-bar.
X_dbl_bar = mean(data_q1$X_bar)
R_bar = mean(data_q1$R)

# Calculate standard deviation of all the observations
Stdv = sd(as.matrix(data_q1[,3:10]))

# Output values for reference
X_dbl_bar
R_bar
Stdv
```


```{r}


# control chart constants
A_2 = 0.373
D_4 = 1.864
D_3 = 0.136

# Calculate the upper control limit and lower control limit for the X-bar chart. Calculate the Center of the two limits.
UCLx = X_dbl_bar + (A_2 * R_bar)
LCLx = X_dbl_bar - (A_2 * R_bar)
CenterX = (UCLx+LCLx)/2

# Calculate the upper control limit and lower control limit for the R chart. Calculate the Center of the two limits.
UCLr = D_4 * R_bar
LCLr = D_3 * R_bar
CenterR = (UCLr+LCLr)/2

# Output values for reference
values <- c(UCLx, LCLx, UCLr, LCLr)
control.limits <- c('UCLx', 'LCLx', 'UCLr', 'LCLr')
data.frame(control.limits, values)
```

The control limits are as follows:
UCLx	39.417750			
LCLx	4.062534			
UCLr	88.340647			
LCLr	6.445455				

### B. Plot X-Bar and R-Bar charts, ensuring that the Upper Control Limit (UCL) and Lower Control Limit (LCL) are clearly indicated on each graph. (8 points)
Identify any points that fall outside the control limits on both the X-Bar and R-Bar(range) charts. Discuss what it means for a point to fall outside the control limits.
Evaluate whether the cement production process is in control based on the control charts.

```{r}
## Plot the R Chart
ggplot(data_q1, aes(x=Date, y=R)) + geom_point() + geom_line() + geom_line(aes(y=LCLr), color="blue") + geom_line(aes(y=UCLr), color="red") + geom_line(aes(y=CenterR), color="orange")+ ggtitle("R Chart")
```

```{r}
sum(data_q1$R > UCLr)
sum(data_q1$R < LCLr)
```


```{r}
ggplot(data_q1, aes(x=Date, y=X_bar)) + geom_point() + geom_line() + geom_line(aes(y=LCLx), color="blue") + geom_line(aes(y=UCLx), color="red") + geom_line(aes(y=CenterX), color="orange") +ggtitle("X-Bar Chart")
```
```{r}
above = which(data_q1$X_bar > UCLx)
data_q1$X_bar[above]
above

```
## ANSWER 
Evaluate whether the cement production process is in control based on the control charts.

The R-Bar chart has all values within the limits, thus it is in control. This means it's only exhibiting common causes of variation.

The X-bar chart has two points falling above the control limit UCL: dates 21 at 41.181 and 40 at 44.767 The process is out of control, meaning there are assignable causes of variation present.

The cement production process is out of control because two points are above the UCLx limit.

### C. If the specification limits were narrowed (e.g., from 30-50 MPa to 35-45 MPa), how would this impact the Cp and Cpk values? What would be the implications for the process? (4 points)
NOte: use all data points to calculate standard deviation.

```{r}
# Calculate Cp
lower_spec = list(30,35)
upper_spec = list(50,45)

Stdv = sd(as.matrix(data_q1[,3:10]))

cp_frame = data.frame(Cp = numeric(), Cpk = numeric())

for(i in list(1,2)){
  Cpi = (upper_spec[[i]] - lower_spec[[i]])/(6*Stdv)
  print(Cpi)
  Cpki = min(((upper_spec[[i]] - X_dbl_bar)/(3*Stdv)), ((X_dbl_bar - lower_spec[[i]])/(3*Stdv)))
  new_row <- data.frame('Cp'=Cpi, 'Cpk'=Cpki)
  cp_frame = rbind(cp_frame, new_row)
}

row.names(cp_frame) <- c('original','narrow')
cp_frame

```
## Answer
Cp is the process capability ratio. With the original window, the CP is less than one, indicating the process is not capable of meeting the upper and lower specification. When the window is more narrow, the value of Cp decreases, meaning that the process is LESS capable of meeting the upper and lower specification, due to the tightened restrictions.

Process capability index, Cpk, gives the proprtion of variation between the center of the process and the upper and lower spec. Values for both the original and more narrow margins are less than 0, meaning the process is not capable of meeting the requirement. Due to just common causes of variation, the process is going to create product that is outside your specifications. The value of Cpk is lower in the narrower margin, meaning it is LESS capable of meeting the requirement. It may indicate that just from common causes, the product will fall outside specifications.

##Question 2 (20 points)
You have recently started your role as the Transportation Director in Mountainside Village, which has only one road for entering and exiting. During peak hours, traffic can reach up to 35 cars per hour, and it takes about 1 minute to drive this road without traffic.

###A. Given that the car arrivals are Poisson-distributed and the drive times are exponential, calculate the current utilization rate of the road. (4 points)

```{r}

lambda = 35
mu = 60

rho = lambda/mu
rho

```
## Answer
The current utlization rate is 0.583.

### B. Given the current traffic conditions, what is the probability that 18 cars will arrive within a 20-minute period?(4 points)

```{r}
P18 = (((lambda*(1/3))**18)*exp(-lambda*(1/3)))/factorial(18)
P18
```
## Answer
The probability that exactly 18 cars will arrive within a 20 minute period is 0.0215 or 2.15%.

### C. Assuming the traffic flow increases to 45 cars per hour during peak periods, how does the new utilization rate compare to the previous rate? (4 points)

```{r}
lambda2 = 45

rho2 = lambda2/mu
rho2
```
## Answer
The new utilization rate is 0.75 or 75%, which is much higher than the utilization rate with 35 cars/hour.

### D. Create a graph to show how the time it takes to travel increases as more cars are on the road. (4 points)¶
Hint: Construct a graph to illustrate the relationship between the number of cars per hour (X-axis), ranging from 35 to 45, and the corresponding time spent on the road (Y-axis).

```{r}
cars_per_hour = 35:45

time_on_road = list()

for(i in cars_per_hour){
  Ws = 1/(mu-i)
  time_on_road = append(time_on_road,Ws*60)
}
time_on_road = unlist(time_on_road)
ggplot(mapping = aes(cars_per_hour, time_on_road))+geom_point() + geom_line()

```

## Answer
The graph above shows that an increase in the cars per hour results in a non-linear increase on the time on road.

### E. A new traffic light has been installed, extending the travel time across the road to 10 minutes. Calculate the average number of cars present on the road at any given time. (4 points)
Note: Use the original arrival rate 35 cars/hour.

```{r}
newmu = 6

cars_on_road = lambda/(newmu - lambda)
cars_on_road

```
## Answer
With the new traffic light, there are more cars entering the system than the road can handle, seen as the utilization rate is 5.83, indicating an overloaded and unstable state. The value for $L_s$ is -1.21. This is not a practical system for this road.


## Question 3 (20 points)
The dataset store_sales.csv comprises a subset of data collected from a U.S.-based retailer between 2010 and 2012. It details weekly sales figures, with each row representing a distinct week of data for a specific store and department.

Notes: The dataset includes periods from 2010 to 2012, but it does not cover every week. Do NOT extrapolate to fill in the missing weeks unless specifically instructed, as it will alter the results. Assume we are using the full dataset for most of the questions.

store: store location id
dept: store department id
date: first day of given week
weekly_sales: sales for that week, dept, store
isholiday: binary variable, true indicates a holiday week.

```{r}
## Load necessary library and dataset
library(dplyr)
library(xts) 
library(forecast) 
library(lubridate) 
# data <-read.csv("../resource/asnlib/publicdata/store_sales.csv", stringsAsFactors = FALSE) 
data <-safe_read_csv("store_sales.csv", stringsAsFactors = FALSE) 
names(data) <- tolower(names(data))
head(data)
```

###A. What is the average value of weekly_sales for dept 1 when there is no holiday? (2 points)
Round your answer to the nearest dollar (e.g., $64.5 rounds up to $65, while $64.49 rounds down to $64).

```{r}

dept1 = data[data$dept == 1,]
dept1_average_weekly_sales = mean(dept1[dept1$isholiday==FALSE,]$weekly_sales)
round(dept1_average_weekly_sales)

```
## Answer
The average weekly sales for dept 1 is $19903 when there is no holiday.

###B. Compute the total sales (sum of weekly_sales across all departments) per week for store 1. Use an alpha of 0.5 and h = 100, and apply the ses() function from the 'forecast' package to fit the data. Plot the model, ensuring that holidays are not excluded. What do you observe in the graph? (4 points)
Note: You will need to convert the dataframe to an xts object.

Follow these steps:

Make all dates to the same format. Hint: use the format parse_date_time(data_columns, c(datetype1, datetype2)).
Convert the dataframe to xts.
Fit the ses() model

```{r}
data <-safe_read_csv("store_sales.csv", stringsAsFactors = FALSE) 
names(data) <- tolower(names(data))
## date format
data$date = parse_date_time(data$date, orders = 'mdy')

```


```{r}
library(scales)

## store1 sales
store1 = data[data$store==1,]
store1_sale = store1 %>%
  group_by(date) %>%
  summarise(weekly_sales = sum(weekly_sales))

## fit the data
## Convert to time-series
dept1_sum_weekly_sales = xts(store1_sale$weekly_sales, order.by = as.Date(store1_sale$date))

# Create model. h is the number of periods to forecast
store1_fit = ses(dept1_sum_weekly_sales, alpha=0.5, h=100)

# Plot model

my_date_transform <- function(x) {format(as.Date(x)+14645, "%m/%d/%Y")}

autoplot(store1_fit) + scale_x_continuous(labels = my_date_transform, n.breaks=15) +  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
## Answer

First looking at the historical data, the sale levels are mainly consistent large spikes around Thanksgiving and Christmas, corresponding with holiday shopping.There is a very slight increase towards the end, but it may not be enough to be wholly reliable.
The larger forecasting shows uncertainty with the trend, with a stable mean. The deviation from mean to 80% percentile looks even for both positive and negative. Over time, the range of sales expands, showing the increasing in uncertainty as the date is farther from the last data point.


###C. Using the same time series dataset from Q3(b), calculate and report the tracking signal for the models with alpha values of 0.25 and 0.75, with h=100. (4 points)¶
Note: Use your model's fitted values (e.g., model[['fitted']]) as your forecasted values.

```{r}

store1_25 = ses(dept1_sum_weekly_sales, alpha=0.25, h=100)
store1_75 = ses(dept1_sum_weekly_sales, alpha=0.75, h=100)

forecast_25 = store1_25$fitted
forecast_75 = store1_75$fitted

actual = store1_sale$weekly_sales

tracking_signal <- function(forecasted_values, actual_values) {
  errors <- forecasted_values - actual_values
  MAD <- mean(abs(errors))
  tracking_signal_value <- sum(errors) / MAD
  return(tracking_signal_value)
}

ts_25 = tracking_signal(forecast_25, actual)
ts_75 = tracking_signal(forecast_75, actual)

paste('Alpha: 0.25 =', round(ts_25,3))
paste('Alpha: 0.75 =', round(ts_75,3))

```

## Answer

The tracking signal is a measure of how often the estimations have been above or below the actual value. The tracking signal for alpha: 0.25 -> -1.044, and for alpha: 0.75 -> 1.652053. This means that for alpha=0.25, most actual values are BELOW the forcast becasue it is negative. For alpha:0.75, most actual values are ABOVE the forecast.

### D. Using the same time series dataset from Q3(b), calculate and report the Mean Absolute Deviation (MAD) and Mean Forecast Error (MFE) for the models with alpha values of 0.25 and 0.75. (4 points)¶
Note: Use you model's fitted values (e.g., model[['fitted']]) as your forecasted values.

```{r}
MAD <- function(forecasted_values, actual_values) {
  errors <- forecasted_values - actual_values
  mad <- mean(abs(errors))
  return(mad)
}

MFE <- function(forecasted_values, actual_values) {
  errors <- forecasted_values - actual_values
  mfe <- sum(errors) / length(actual_values)
  return(mfe)
}

mad_25 = MAD(forecast_25, actual)
mad_75 = MAD(forecast_75, actual)

mfe_25 = MFE(forecast_25, actual)
mfe_75 = MFE(forecast_75, actual)

paste('MAD, alpha: 0.25 =', round(mad_25))
paste('MAD, alpha: 0.75 =', round(mad_75))
paste('MFE, alpha: 0.25 =', round(mfe_25))
paste('MFE, alpha: 0.75 =', round(mfe_75))

```

## Answer
MAD is the average absolute error in the observation. MFE is the average error in the observation.
MAD, alpha: 0.25 = 115457
MAD, alpha: 0.75 = 118502
MFE, alpha: 0.25 = -1030
MFE, alpha: 0.75 = 1673

### E. Based on your previous calculations, using the Tracking Signal, MAD, and MFE values for the models with alpha values of 0.25 and 0.75, determine which model performs better and explain why. (2 points)¶

## Answer
According to tracking signal, 0.25 performs better, as -1.044 is closer to zero than 1.652, and a perfect tracking signal is 0.
According to MAD, 0.25 also performs better. A smaller value indicates smaller mean absolute deviation, the distance away from the actual value.
According to MFE, 0.25 performs better. A value closer to 0 means less Mean Forecast Error, thus less error and a better model.
Alpha=0.25 creates a better performing model becasue it outperforms alpha=0.75 by tracking signal, MAD, and MFE metrics.

### F. Using the same time series dataset from Q3(b), calculate and plot the root mean squared error (RMSE) for alpha values ranging from 0.01 to 0.99 in increments of 0.01, with h=100. Identify the optimal alpha value that yields the lowest RMSE. (4 points)
Note:

Use the seq() function to generate a sequence of alpha values and loop through it to calculate RMSE.
The MSE value can be obtained from your SES model using yourmodelname$model$mse.

```{r}

alphas = seq(.01,.99,.01)
RMSE = list()

for(i in alphas){
  modeli = ses(dept1_sum_weekly_sales, alpha=i, h=100)
  RMSEi = sqrt(modeli$model$mse)
  RMSE = append(RMSE, RMSEi)
}

RMSE = unlist(RMSE)
alpha_min = alphas[which.min(RMSE)]
RMSE_min = min(RMSE)
new_point = data.frame(alpha=alpha_min, RMSE=RMSE_min)

ggplot(mapping = aes(x = alphas, y = RMSE)) +
  geom_line() +
  geom_point(data = new_point, aes(x = alpha, y = RMSE), color = 'red', size = 3) +
  geom_text(data = new_point, aes(x = alpha, y = RMSE, label = paste("(",(alpha_min),",", round(RMSE_min),")")
), color="red",vjust = -1.5, hjust = 0.2, size = 4) +
  labs(x = "Alpha", y = "RMSE", title = "RMSE vs. Alpha for SES")

```

## Answer
The optimal value of alpha is 0.08, which gives a RMSE of 165410.
