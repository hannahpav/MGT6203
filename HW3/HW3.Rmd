---
title: "HW3"
output: html_document
date: "2024-06-30"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

## saferead

```{r saferead}
# The safe_read_csv function is used for checking datasets.
safe_read_csv <- function(file_path, stringsAsFactors = TRUE) {
    result <- tryCatch({
        data <- read.csv(file_path, stringsAsFactors = stringsAsFactors)
        mes <- paste(file_path, " successfully read.")
        print(mes)
        return(data)
      }, warning = function(w) {
    # Warning Message
        message("Warning: ", conditionMessage(w))
        message("Using alternative file path.")
        # Use regular expression to get file name after last slash
        file_name <- sub("^.*/", "", file_path) 
        # See if file in first layer of working directory.
        # If not, student can easily upload it to Vocareum directory.
        result <- read.csv(file_name, stringsAsFactors = stringsAsFactors)
        print("Alternate path successful")
        return(result)
    }, error = function(e) {
    # If neither method was successful
        message("Error: ", conditionMessage(e))
        print("Please upload data to Vocareum")
      }, finally = {
        ## This message appears regardless of whether an error appears
        final_mes <- paste("If ", file_path, " not found, please notify us in Piazza. Thank you.")
        print(final_mes)
    }) 
    return(result)
}
```

```{r}
personal_tiingo_api_key = '75b478897440f63dfd9ed6915f72f30e832defb0'
tiingo_api_key(personal_tiingo_api_key)
```


## Instructions for Question 1 (16 Points)
For Question 1, the goal is to use the data to construct a three-factor model. We'll do this by looking at how returns for UPS and KO, after accounting for the risk-free rate, relate to three factors—Mkt_RF, SMB, and HML—during our study period. Specifically, you will employ regression analysis to conduct factor investing for UPS and KO stocks.

### Please use the dataset 'UPS_KO.csv' to answer the following questions.
The data set contains the following columns:

Date: This column represents date from 09/2014 to 08/2019.
Mkt_RF: This column represents market premium (i.e., Market return – risk_free rate).
SMB: This column represents the value of the size factor.
HML: This column represents the value of the value factor.
RF: This column represents risk free rate.
UPS: This column represents the daily return of UPS.
KO: This column represents the daily return of KO.
Sample Period : 04/2015 to 11/2018 (Inclusive)

```{r loaddata}
# Load the data and necessary library
suppressPackageStartupMessages(library(dplyr))
library(dplyr)

# Read the dataset
# data_Q1 <- read.csv("../resource/asnlib/publicdata/UPS_KO.csv")
data_Q1 <- safe_read_csv("../resource/asnlib/publicdata/UPS_KO.csv")

# Format the data
data_Q1 <- data_Q1 %>% mutate(Date = as.Date(paste0(Date, "01"), format = "%Y%m%d"))

# Define the start and end dates for filtering
start_date <- as.Date("2015-04-01")
end_date <- as.Date("2018-11-30")

# Filter the dataframe based on the date range
filtered_df <- data_Q1 %>%
  filter(Date >= start_date & Date <= end_date) %>% 
  arrange(Date)

filtered_df$UPS_RF <- filtered_df$UPS - filtered_df$RF
filtered_df$KO_RF <- filtered_df$KO - filtered_df$RF


tail(filtered_df)
```
## A. Using the dataset named filtered_df provided above, fit a factor regression model for UPS over the sample period using Mkt_RF, SMB, and HML (6 points).
Report each of the coefficients and interpret each variable (including intercept) according to their financial interpretation AND taking statistical significance into account at alpha=0.1.

```{r}

model1 = lm(UPS~ Mkt_RF + SMB + HML, data = filtered_df)
summary(model1)

```
The intercept is close to zero at -7.249e-05 and negative, which would mean that the UPS fund slightly underperformed in relation to the market. However, because this intercept in not significantly significant, no conclusion can be made about the overall performance in relation to the market.

Mkt_RF of 1.065 is the only variable that is stastically significant in this model at level 0.01%. The 1.065 means that the beta is 1.065, meaning UPS is slightly more volatile than the market.

The positive coefficient 4.331e-02 on SMB would indicate that UPS is tilted toward small cap stocks. However, this variable is not statistically significant in the model, so no conclusion can be reached.

A positive coefficient on HML would indicate that the UPS fund is titled toward value stocks. The coefficient is not statistically significant at a 0.1% level, so no conclusion can be reached.

## B. Using the dataset named filtered_df provided above, fit a factor regression model for KO over the sample period using Mkt_RF, SMB, and HML (6 points).
Report each of the coefficients (including intercept) and interpret each according to their financial interpretation AND taking statistical significance into account at alpha=0.1



```{r}

model2 = lm(KO~ Mkt_RF + SMB + HML, data = filtered_df)
summary(model2)

```
The intercept is positive, which would mean that KO fund overperformed in relation to the market. However, because this intercept in not significantly significant, no conclusion can be made about the overall performance in relation to the market.

Mkt_RF of 0.460956 is stastically significant in this model at level 0.01%. The 0.460956 means that the beta is 0.460956, meaning KO is slightly more volatile than the market.

The negative coefficient -0.799243 on SMB indicates that UPS is tilted toward large cap stocks. The variable is statistically significant at a 0.05% level, so the interpretation holds.

A positive coefficient on HML would indicate that the UPS fund is titled toward value stocks. The coefficient is not statistically significant at a 0.1% level, so no conclusion could be reached.

## C. Based on Warren Buffett's investment philosophy discussed in our course readings, which company is he more likely to invest in according to these factor model outputs? (4 points)

He would prefer KO because it has a low beta, and Buffet prefers to invest in low-risk stocks, or "safe" stocks.
Buffet also prefers value stocks or "cheap" stocks, which are indicated by a positive HML coefficient. For both KO and UPS, the coefficients are positive, with KO at a higher coefficient than UPS. However, neither coefficient is statistically significant, so HML is not a good measurement for his choice.
Lastly, he prefers profitable stocks, which is indicated by coefficient QMJ. which is not included in this model.

Ultimately, Buffer would prefer KO because of its low risk.

# Instructions for Question 2 (32 Points)
For Question 2, you will use logistic regression to segment customers into two classes. This question is adapted from a Kaggle contest to evaluate current customers for an auto dealership that is opening a new location. The dealership wishes to reliably segment customers in order to streamline marketing efforts when their new location opens.

The data for these questions is contained in the file, auto_customers_two_segments.csv, which includes the following fields:

ID : Unique ID created by dealership (Removed from data_Q2)
Gender : Gender of the customer
Ever_Married : Marital status of the customer
Age : Age of the customer
Graduated : Is the customer a college graduate?
Profession : Profession of the customer
Work_Experience : Work experience in years
Spending_Score : Spending score of the customer.
Family_Size : Number of family members (including customer)
Segmentation : The marketing segment the customer is in.

```{r}
# Load necessary library and data
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(knitr))
library(caret)

#load data from file
data_Q2 = safe_read_csv('data/auto_customers_two_segments.csv')
head(data_Q2 )
```

## A. Follow the steps below to clean the data set and answer the question. (4 Points)
Remove all rows with n/a values.
Remove all rows with data points that are blank (these are "" values that have a string length of 0). You can use dplyr or other methods to complete this step. (HINT: the cleaned sample size is 3400)
What are the customer segment labels (Segmentation column) and how many customers are in each segment?

```{r}

# Format the data
filtered_q2 <- data_Q2 %>% drop_na() %>%
  filter_all(all_vars(str_length(as.character(.)) != 0))

length(filtered_q2$Age)
```

```{r}
filtered_q2 %>% group_by(Segmentation) %>% count()
```

There are two groups, A and B. There are 1628 customers in A and 1772 customers in B

#B. Create a logistic regression model using Segmentation as the response variable and all other variables as the predictors. (4 Points)
Segment A represents the most desirable customers
Segment B represents customers who have made purchases, but are not as likely to have a high or average spending score.
HINT: You need to specify Segment B as a reference level by setting Segment B to 0 and Segment A to 1.

```{r}

filtered_q2$Segmentation <- ifelse(filtered_q2$Segmentation == "A", 1, 0)

model3 <- glm(Segmentation ~., data = filtered_q2, family = 'binomial')
summary(model3)

```

## C. Use the cleaned data set for both training and testing data and make predictions of the probability for each customer. (4 Points)
For simplicity, we are using the same dataset for both training and testing.
Display the first 5 values. Note: round your results to three decimal places, e.g., XX.xxx.
HINT: You can use the predict() function on the model with the dataset to generate a vector of probability predictions.

```{r}
filtered_q2$model_prob <- predict(model3, filtered_q2[,-9], type = "response")

head(round(filtered_q2$model_prob,3), 5)
```

The first five values for probabilities of customers are: 0.052, 0.174, 0.821, 0.579, 0.046.

## D. Predict the accuracy of the model for each of the following cutoff probability values. (4 Points)
Classify each predicted value as 1 (representing Segment A) or 0 (representing Segment B), using the following cutoff probability values: 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8.
Calculate accuracy:
Accuracy=(TP+TN)(TP+TN+FP+FN)
 
Round your results to three decimal places, e.g., XX.xxx.
Note: You will need to calculate the accuracy seven times, once for each cutoff value.

```{r}
library(xtable)
cut_list = list(0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8)
accuracy_list = list()

for (cut in cut_list) {
filtered_q2_1 <- filtered_q2 %>% 
  mutate(model_predict = case_when(filtered_q2$model_prob>cut~1,TRUE~0))

filtered_q2_1 <- filtered_q2_1 %>% 
  mutate(Segmentation = as.factor(Segmentation)) %>%
  mutate(model_predict = as.factor(model_predict))

Confusion_mat = caret::confusionMatrix(filtered_q2_1$model_predict, filtered_q2_1$Segmentation ,positive='0')

accuracy_list = append(accuracy_list, round((Confusion_mat$overall[[1]]),4))

}

cutoff_table = data.frame('cutoff' = (unlist(cut_list)), 'accuracy'= unlist(accuracy_list))
xtable(cutoff_table, type='latex')
cutoff_table %>% mutate(accuracy = round(accuracy,3))

```
\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
 & cutoff & accuracy \\ 
  \hline
1 & 0.20 & 0.69 \\ 
  2 & 0.30 & 0.73 \\ 
  3 & 0.40 & 0.75 \\ 
  4 & 0.50 & 0.76 \\ 
  5 & 0.60 & 0.74 \\ 
  6 & 0.70 & 0.70 \\ 
  7 & 0.80 & 0.62 \\ 
   \hline
\end{tabular}
\end{table}

## E. Which cutoff probability value ryields the highest accuracy for the model? What is the accuracy as a percentage? (4 Points)
Round percentages to two decimal places, for example, 80.51%

```{r}
cutoff_table[which.max(cutoff_table$accuracy),]
```

The highest accuracy is at cutoff 0.5 with 75.74% accuracy

## F. Now we have developed a model and found the model that produces the highest accuracy. Create a confusion matrix using predictions from the optimal cutoff probability value above. (4 Points)
HINT: You may use the caret package function, confusionMatrix() to automate this process

```{r}
filtered_q2_2 <- filtered_q2 %>% 
  mutate(model_predict = case_when(filtered_q2$model_prob>.5~1,TRUE~0))

filtered_q2_2 <- filtered_q2_2 %>% 
  mutate(Segmentation = as.factor(Segmentation)) %>%
  mutate(model_predict = as.factor(model_predict))

Confusion_mat_opt = caret::confusionMatrix(filtered_q2_2$model_predict, filtered_q2_2$Segmentation ,positive='1')

Confusion_mat_opt$table

```
## G. Report the accuracy, sensitivity and specificity of the chosen model as percentages. (4 Points)
Round percentages to two decimal places, for example, 80.51%

```{r}
accuracy = round(100*Confusion_mat_opt$overall[[1]],2)
sensitivity = round(100*Confusion_mat_opt$byClass[[1]],2)
specificity = round(100*Confusion_mat_opt$byClass[[2]],2)

setNames(data.frame(c('accuracy', 'sensitivity', 'specificity'), c(accuracy, sensitivity, specificity)), c('measure', 'value'))

```

## H. Evaluate your model. (4 Points)
After you shared your results with the auto dealership's leadership team, they expressed concerns about the high number of customers from Segment A being misclassified as Segment B.

Please address the following specific questions:

Why is misclassifying Segment A customers as Segment B problematic? What implications does this have?
How would you adjust your model to more accurately classify Segment A customers?
What are the potential downsides of making these adjustments to your model?

### Answer
Misclassifying Segment A customers as Segment B too often means that when true value = 1, model is predicting value = 0, which is a false negative, which means that the model has low sensitivity. This means that the model is failing to identify Segment A. From a sales perspective, Segment A could be repeatedly getting Segment B's advertisement. If Segment A is the segment that does more purchasing, that means that the company is losing sales.

To adjust the model to more accurately classify Segment A customers, we could decrease the threshold value p=0.5. By decreeasing this, more instances are predicted as positive (1), thus increasing the number of Segment A's predicted.

When adjusting the threshold, one must be accurate to not lower the threshold so much that the accuracy suffers. The model built in this question is optimized for accuracy. If we were to change the threshold, the accuracy would be less. The user would have to decide which was more important, the sensitivity or the accuracy.

# Instructions for Question 3 (12 points)
Company ABC is investing in Facebook, Pinterest, Twitter, and online display advertising to boost sales.
To evaluate the effectiveness of their marketing budget, we are tasked with developing a straightforward Media Mix Model using linear regression methods.

The data for these questions is contained in the file, mmmdata.csv, which includes the following fields:

Date : Date.
Sales : Sales of Company ABC.
Facebook : Marketing spending on Facebook.
Pinterest : Marketing spending on Pinterest.
Twitter : Marketing spending on Twitter.
Online_Display : Marketing spending on Online_Display.

```{r}
## Load dataset 
data_Q3 = safe_read_csv("data/mmmdata.csv", stringsAsFactors = FALSE)

## AdStock Terms
data_Q3$Facebook.adstock <- as.numeric(stats::filter(x= data_Q3$Facebook , filter =0.3 , method ="recursive"))
data_Q3$Pinterest.adstock <- as.numeric(stats::filter(x= data_Q3$Pinterest , filter =0.2 , method ="recursive"))
data_Q3$Twitter.adstock <- as.numeric(stats::filter(x= data_Q3$Twitter , filter =0.7 , method ="recursive"))
data_Q3$Online_Display.adstock <- as.numeric(stats::filter(x= data_Q3$Online_Display , filter =0.5 , method ="recursive"))

head(data_Q3)
```

## A. Please develop the following model. (4 points)
Model 1 : Regress Sales on Company ABC’s Facebook, Pinterest, Twitter, and Online Display spending.

```{r}
model4 = lm(Sales ~ Facebook + Pinterest + Twitter + Online_Display, data = data_Q3)
summary(model4)

```

## B. Please develop the following model. (4 points)¶
Model 2 : Regress Sales on the adstock terms defined in the instruction. DO NOT make any changes to the .adstock variables. ONLY include the .adstock variables listed in your analysis.

```{r}
model5 = lm(Sales ~ Facebook.adstock + Pinterest.adstock + Twitter.adstock + Online_Display.adstock, data = data_Q3)
summary(model5)
```

## C. Explain which model best explains the data and why? (4 points)

Model2, which is regressed on adstock terms, best explains the model. 
The ad stock variable is created by computing the exponential decay of the impressions  on each day and then summing up the total "stock" from impressions on previous days. Basically, it describes the prolonged or lagged effect of advertising on consumer purchase behavior. 
The ad stock model, then take into account the decay over time that ads have. For this reason, the model can capture effect from ads that may have a lag time.
Because of the time-dependent nature of the model and historical context, it can improve forecasting accuracy.
The model is also more effective at investigating the relationship between total sales and advertising channels, which may be lost in the raw data, allowing for a synergistic model.

From a completely non-adstock perspective, the adjusted R-squared for the adstock model is slightly higher than that of the raw data model, indicating the adstock model better explains the response.
